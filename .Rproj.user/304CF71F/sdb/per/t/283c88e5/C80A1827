{
    "collab_server" : "",
    "contents" : "---\ntitle: \"sidtoolbox_onearm\"\noutput: rmarkdown::html_vignette\nvignette: >\n  %\\VignetteIndexEntry{sidtoolbox_onearm}\n  %\\VignetteEngine{knitr::rmarkdown}\n  \\usepackage[utf8]{inputenc}\n---\n\n# Subgroup discovery demonstration\n\n```{r load_packaged, message=FALSE, warning=FALSE, include=FALSE}\nlibrary(sidtoolbox)\nlibrary(data.table)\nlibrary(stringr)\nlibrary(ggplot2)\n\n```\n\n## Simulated data\n\n\n```{r simulate_data}\n\n# simulate a sample dataset with 3 types of variables, numeric, binary, and count\n# covariates are X1, X2, ...\n# outcomes are Y_[type], namely Y_numeric, Y_binary, and Y_count\n# the contrast arm is TRT\n# 'N' samples will be created \n# also places two subgroups, oneis true positive (random_TP) of size 'subgroup_ratio'*'N' where the treatment arm is enhanced by 'subgroup_enhanced_effect'\n# and false positive (random_FP) of same size but where both arms are shifted by 'subgroup_shift_effect'\nsubgroup.data <- simulateSubgroupData(\n                            N = 2000,\n                            case = \"prognostic\", \n                            arm = \"one\",\n                            overal_treatment_effect = 0.5, \n                            subgroup_enhanced_effect = 1.5, \n                            subgroup_ratio = 0.1,\n                            covariates_normal = list(c(0,1), c(0,1), c(0,1)),\n                            correlation = c(),\n                            covariates_binary = c(0.5, 0.5, 0.5, 0.5), \n                            covariates_uniform = list(c(0,1), c(0,1), c(0,1)), \n                            has_FP = TRUE,\n                            SD_NOISE_RATIO = 0.1)\n# to load your own data use loadDataset function\n# you would need to set your covariates and outcomes.\n# for one arm study do not set the contrast parameter.\n#loadDataset(dataset = subgroup.data, \n#            covariates = subgroup.data$covariates, \n#            outcomes = subgroup.data$outcomes, \n#            outcomeTypes = subgroup.data$outcomeTypes)\n\n```\n\nThen set some global variables. The minimum size of a subgroup to be found, how deep the rules can go, the significance threshold, and the smallest value for p-value to avoid underflow.\n\n```{r set_global_variables}\nSUBGROUP_MIN_SIZE <- 0.1\nMAX_DEPTH <- 2\nP_THRESHOLD <- 0.05\nMIN_P_VALUE <- 10^-10 # to avoid underflow\n\n```\n\n\n## Explore the data\nThis step helps identify covariates to be included in the subgroup discovery.\n\n### Correlation matrix\nCorrelations who one on one relation in your data. \nHighly correlated covariates should be removed.\nHere we get the corrlation between numeric features.\n\n```{r exploreCorrelationMatrix, message=FALSE, warning=FALSE}\n\n# examples\nexploreTriangleCorrelationMatrix(subgroup.data = subgroup.data, method = \"spearman\")\nexploreCorrelationMatrix(subgroup.data = subgroup.data)\nexploreDetailedChartCorrelation(subgroup.data = subgroup.data)\n\n\n```\n\n### Association rules\nAssociation rules help us find high level correlations in the data. \nThe rules are controled by support (how large the corresponding subgroup is) and confidence (how many of the total subgroup size follow the rule).\n\nAssociation rules are defined by support and confidence. Support shows us how frequently the rule (left hand side) is in the data, similar to subgroup size. Confidence is the number of times the rule is correct (right hand side | left hand side), and thus is a measure of strength for the rule. The minimum and maximum depth of the rule can be set by min_order and max_order.\n\nBy filtering RHS (right hand side) for the outcome variables you can find immediate subgroups with highest support.\n\n```{r exploreAssociationRules, message=FALSE, warning=FALSE}\n\n# res <- exploreAssociationRules(subgroup.data)\nmin_support <- 0.1 # how many samples support the LHS\nmin_confidence <- 0.5 # how many times RHS | LHS is true\nmin_order <- 2 # minimum depth of rules\nmax_order <- 4 # maximum depth of rules\npaste(\"minimum support =\", min_support, \n      \"minimum confidence =\", min_confidence,\n      \"minimum depth =\", min_order,\n      \"maximum depth =\", max_order)\n\na <- capture.output(p <- plotAssociationRules(subgroup.data = subgroup.data, onlyCovariates = TRUE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))\np\n\na <- capture.output(p <- plotAssociationRules(subgroup.data = subgroup.data, method = \"graph\", measure = \"confidence\", onlyCovariates = TRUE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))\np\n\na <- capture.output(p <- plotAssociationRules(subgroup.data = subgroup.data, method = \"scatterplot\", measure = \"support\", onlyCovariates = TRUE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))\np\n\na <- capture.output(p <- plotAssociationRules(subgroup.data = subgroup.data, method = \"two-key plot\", measure = \"support\", onlyCovariates = FALSE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))\np\n\n#plotAssociationRulesInteractive(subgroup.data = subgroup.data, onlyCovariates = TRUE)\na <- capture.output(rules <- listAssociationRules(subgroup.data = subgroup.data, onlyCovariates = FALSE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))\n\nas.data.table(rules)\n\n```\n\n### Conditional inference tree\nConditional inference trees are supervised trees useful to find different subgroups. \nIn a one arm study, the leaves can be intrepreted as subgroups.\nYou can see one tree for each outcome. \n\n<b>Be cautious when intrepreting the survival data as the values will be treated as numeric. </b>\n\n\n```{r exploreDecisionTree, message=FALSE, warning=FALSE}\n# on all coavriates\na <- lapply(X = subgroup.data$outcomes, FUN = function(y) {\n  capture.output(p <- exploreConditionalInferenceTree(subgroup.data = subgroup.data, y_idx = y, stump = FALSE, testtype = \"Bonferroni\", significance = P_THRESHOLD, nmin = SUBGROUP_MIN_SIZE, maxdepth = MAX_DEPTH))\n  plot(main = paste(\"all covariates - \", colnames(subgroup.data$data)[y]), p)\n})\n\n```\n\n\nYou can use the above information to filter down your features. User knowledge is required to decide which features to eliminate. Note that correlated features will cause errors in the downstream analysis.\n\n## Subgroup discovery\nLet us discover some subgroups.\n\n```{r subgroup_discovery, message=TRUE, warning=TRUE}\n# set the outcomes you want to run subgroup discovery on\noutcome_columns <- colnames(subgroup.data$data)[subgroup.data$outcomes]\n# save all rules in allRules\nallRules <- subgroup.data$rules\nallRules\n\n```\n\n### Treatment-specific Subgroup Discovery Tool (TSDT)\n<b>\nBattioui, C., Shen, L., Ruberg, S., (2014). <i>A Resampling-based Ensemble Tree Method to Identify\nPatient Subgroups with Enhanced Treatment Effect.</i> JSM proceedings, 2014 </b>\n\nIncrease the number of CPU cores according to your machine.\nIncrease the sample and permutation in a real world study to at least 100 and 100. The suggested setting by authors is 100 and 500.\n\n<font color=\"red\">Caution: TSDT does not have a built-in model for count data. It assumes it is numeric, and the package adds log transformation on it.</font>\n\n```{r TSDT, message=TRUE, warning=TRUE}\n# the true positive / false positive subgroup is\n# subgroup.data$rules\n\nparam <- list(\n  desirable_response = \"increasing\",\n  nsamples = 100, \n  npermutations = 100, # increase the permutation in a real world study \n  maxdepth = MAX_DEPTH, \n  min_subgroup_n_control = SUBGROUP_MIN_SIZE/2, \n  min_subgroup_n_trt = SUBGROUP_MIN_SIZE/2, \n  n_cpu = 1 # increase the number of CPU cores according to your machine\n)\n\nresTSDT <- lapply(X = outcome_columns, FUN = function(y_idx) {\n  res <- runTSDT(subgroup.data = subgroup.data, \n                     y_idx = y_idx,\n                     desirable_response = param$desirable_response,\n                     nsamples = param$nsamples, \n                     npermutations = param$npermutations, \n                     maxdepth = param$maxdepth,\n                     n_cpu = param$n_cpu)\n  allRules <<- rbind(allRules, \n                    parseTSDTResults(TSDT_table = res, outcome = y_idx, param = param, filter = \"Strong\"))\n  return(res)\n})\n\nresTSDT\n\n```\n\n\n### Optimization approach with Particle Swarm Optimization (PSO)\nPSO searches for optimal cutoffs while leaxing for p-value and depth. \nYou can define your own function or use the default model:\n<b>effect size: Y ~ subgroup for one-arm</b> and \ninteraction effect size: Y ~ contrast + subgroup + contrast*subgroup for two-arm.\n\nUse larger iteration numbers for larger datasets. 1000 should suffice for up to 100 covariates.\n\n```{r PSO, message=TRUE, warning=TRUE}\n\n\n# increasing\nresPSO <- lapply(X = outcome_columns,\n                 FUN = function(y_idx){\n  pso <- runPSO(subgroup.data = subgroup.data, \n                y_idx = y_idx,  \n                desirable_response = \"increasing\", \n                depth = MAX_DEPTH, \n                nmin = SUBGROUP_MIN_SIZE, \n                iterations = 1000)\n  allRules <<- rbind(allRules, \n                     pso)\n  pso\n})\nresPSO\n\n\n```\n\n\n## Analyze and compare rules\n\n```{r compare_rules, echo=FALSE, message=FALSE, warning=FALSE}\nthreshold <- -log10(P_THRESHOLD)\nmax_p_value <- -log10(MIN_P_VALUE)\nsummarizeRules <- function(subgroup.data, rules) {\n  N.rules <- nrow(rules)\n  summaryRules <- data.table()\n  \n  allOutcomes <- outcome_columns\n  \n  for (i in c(1:N.rules)) {\n    row <- rules[i]\n    # adding a fix to remove dummy rules wihch were negative controls\n    if (!grepl(x = row$rule, pattern = \"dummy\")) {\n      if (str_length(row$outcome) == 0) {\n        Y <- allOutcomes\n      } else {\n        Y <- row$outcome\n      }\n      for (y in Y) {\n        row$outcome <- y\n        summaryRules <- rbind(summaryRules, row)\n      }\n    }\n    \n  }\n  N.rules <- nrow(summaryRules)\n  N <- nrow(subgroup.data$data)\n  effect_size <- sapply(\n    X = c(1:N.rules),\n    FUN = function(i) {\n      row <- summaryRules[i]\n      sbg_idx <-\n        parseRule(covariates = subgroup.data, rule = row$rule)\n      f <- effectSize(\n        subgroup.data = subgroup.data,\n        y_idx = row$outcome,\n        subgroup_idx = sbg_idx\n      )\n      c(\n        N_ratio = sum(sbg_idx) / N,\n        effect = f[1],\n        pvalue = f[2]\n      )\n    }\n  )\n  summaryRules$N_ratio <- effect_size[1, ]\n  summaryRules$effect_size <- effect_size[2, ]\n  summaryRules$pvalue <- effect_size[3, ]\n  \n  # check TP rule and get accuracy precision recall\n  truth_subgroup <- which(rules$method == \"sim_TP\")\n  if (length(truth_subgroup) > 0) {\n    truth_subgroup <- parseRule(covariates = subgroup.data,\n                                rule = rules[truth_subgroup[1], ]$rule)\n    performance <- sapply(\n      X = c(1:N.rules),\n      FUN = function(i) {\n        row <- summaryRules[i, ]\n        sbg_idx <-\n          parseRule(covariates = subgroup.data, rule = row$rule)\n        TP <- sum(sbg_idx &  truth_subgroup)\n        FP <- sum(sbg_idx & !truth_subgroup)\n        TN <- sum(!sbg_idx & !truth_subgroup)\n        FN <- sum(!sbg_idx &  truth_subgroup)\n        precision <- TP / (TP + FP)\n        recall <- TP / (TP + FN)\n        F1  <- 2 * (recall * precision) / (precision + recall)\n        c(precision = precision,\n          recall = recall,\n          F1 = F1)\n      }\n    )\n    summaryRules$precision <- performance[1, ] * 100\n    summaryRules$recall <- performance[2, ] * 100\n    summaryRules$F1 <- performance[3, ] * 100\n  }\n  return(summaryRules)\n}\n\nrules.data <- summarizeRules(subgroup.data = subgroup.data, rules = allRules)\nrules.data$log10pvalue <- -log10(rules.data$pvalue)\nrules.data$log10pvalue[rules.data$log10pvalue > max_p_value] <- max_p_value\n\n\nggplot(\n  data = rules.data,\n  aes(\n    x = precision,\n    y = recall,\n    shape = outcome,\n    color = method,\n    size = log10pvalue\n  )\n) +\ngeom_point() +\ngeom_jitter() + \nggtitle(\"Correctness of rules\")\n\nggplot(\n  data = rules.data,\n  aes(\n    x = effect_size,\n    y = log10pvalue,\n    shape = outcome,\n    color = method,\n    size = N_ratio\n  )\n) +\ngeom_point() +\n  geom_jitter() + \nggtitle(\"Volcano plot of rules\") +\ngeom_hline(yintercept = P_THRESHOLD) +\nxlab(\"subgroup effect size\") + \nylab(\"-log10(pvalue)\")\n\n\nrules.data <- rules.data[, c(\"rule\", \"outcome\", \"N_ratio\", \"effect_size\", \"pvalue\", \"precision\", \"recall\", \"F1\")]\nas.data.table(rules.data)\n\n```",
    "created" : 1565713412224.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3608600449",
    "id" : "C80A1827",
    "lastKnownWriteTime" : 1565725806,
    "last_content_update" : 1565726055132,
    "path" : "~/sidtoolbox/vignettes/demo_onearm.Rmd",
    "project_path" : "vignettes/demo_onearm.Rmd",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}