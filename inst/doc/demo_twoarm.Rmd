---
title: "sidtoolbox_twoarm"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sidtoolbox_twoarm}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---
# Subgroup discovery demonstration

```{r load_packaged, message=FALSE, warning=FALSE, include=FALSE}
library(sidtoolbox)
library(data.table)
library(stringr)
library(ggplot2)

```

## Simulated data


```{r simulate_data}

# simulate a sample dataset with 3 types of variables, numeric, binary, and count
# covariates are X1, X2, ...
# outcomes are Y_[type], namely Y_numeric, Y_binary, and Y_count
# the contrast arm is TRT
# 'N' samples will be created 
# also places two subgroups, oneis true positive (random_TP) of size 'subgroup_ratio'*'N' where the treatment arm is enhanced by 'subgroup_enhanced_effect'
# and false positive (random_FP) of same size but where both arms are shifted by 'subgroup_shift_effect'
subgroup.data <- simulateSubgroupData(
                            N = 2000,
                            case = "prognostic", 
                            arm = "two",
                            overal_treatment_effect = 0.5, 
                            subgroup_enhanced_effect = 1.5, 
                            subgroup_ratio = 0.1,
                            covariates_normal = list(c(0,1), c(0,1), c(0,1)),
                            correlation = c(),
                            covariates_binary = c(0.5, 0.5, 0.5, 0.5), 
                            covariates_uniform = list(c(0,1), c(0,1), c(0,1)), 
                            has_FP = TRUE,
                            SD_NOISE_RATIO = 0.1)

# to load your own data use loadDataset function
# you would need to set your covariates, outcomes and contrast arm (two arm study)
#loadDataset(dataset = subgroup.data, 
#            covariates = subgroup.data$covariates, 
#            outcomes = subgroup.data$outcomes, 
#            outcomeTypes = subgroup.data$outcomeTypes, 
#            trt = subgroup.data$contrast, 
#            ctrl = subgroup.data$control)

```

Then set some global variables. The minimum size of a subgroup to be found, how deep the rules can go, the significance threshold, and the smallest value for p-value to avoid underflow.

```{r set_global_variables}
SUBGROUP_MIN_SIZE <- 0.1
MAX_DEPTH <- 2
P_THRESHOLD <- 0.05
MIN_P_VALUE <- 10^-10 # to avoid underflow

```


## Explore the data
This step helps identify covariates to be included in the subgroup discovery.

### Correlation matrix
Correlations who one on one relation in your data. 
Highly correlated covariates should be removed.
Here we get the corrlation between numeric features.

```{r exploreCorrelationMatrix, message=FALSE, warning=FALSE}

# examples
exploreTriangleCorrelationMatrix(subgroup.data = subgroup.data, method = "spearman")
exploreCorrelationMatrix(subgroup.data = subgroup.data)
exploreDetailedChartCorrelation(subgroup.data = subgroup.data)


```

### Association rules
Association rules help us find high level correlations in the data. 
The rules are controled by support (how large the corresponding subgroup is) and confidence (how many of the total subgroup size follow the rule).

Association rules are defined by support and confidence. Support shows us how frequently the rule (left hand side) is in the data, similar to subgroup size. Confidence is the number of times the rule is correct (right hand side | left hand side), and thus is a measure of strength for the rule. The minimum and maximum depth of the rule can be set by min_order and max_order.

By filtering RHS (right hand side) for the outcome variables you can find immediate subgroups with highest support.

```{r exploreAssociationRules, message=FALSE, warning=FALSE}

# res <- exploreAssociationRules(subgroup.data)
min_support <- 0.1 # how many samples support the LHS
min_confidence <- 0.5 # how many times RHS | LHS is true
min_order <- 2 # minimum depth of rules
max_order <- 4 # maximum depth of rules
paste("minimum support =", min_support, 
      "minimum confidence =", min_confidence,
      "minimum depth =", min_order,
      "maximum depth =", max_order)

a <- capture.output(p <- plotAssociationRules(subgroup.data = subgroup.data, onlyCovariates = TRUE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))
p

a <- capture.output(p <- plotAssociationRules(subgroup.data = subgroup.data, method = "graph", measure = "confidence", onlyCovariates = TRUE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))
p

a <- capture.output(p <- plotAssociationRules(subgroup.data = subgroup.data, method = "scatterplot", measure = "support", onlyCovariates = TRUE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))
p

a <- capture.output(p <- plotAssociationRules(subgroup.data = subgroup.data, method = "two-key plot", measure = "support", onlyCovariates = FALSE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))
p

#plotAssociationRulesInteractive(subgroup.data = subgroup.data, onlyCovariates = TRUE)
a <- capture.output(rules <- listAssociationRules(subgroup.data = subgroup.data, onlyCovariates = FALSE, support = min_support, confidence = min_confidence, min_order = min_order, max_order = max_order))

as.data.table(rules)



```

### Conditional inference tree
Conditional inference trees are supervised trees useful to find different subgroups. 
In a one arm study, the leaves can be intrepreted as subgroups.
You can see one tree for each outcome. 

<b>Be cautious when intrepreting the survival data as the values will be treated as numeric. Count data will be log transformed. </b>


```{r exploreDecisionTree, message=FALSE, warning=FALSE}
# on all coavriates
a <- lapply(X = subgroup.data$outcomes, FUN = function(y) {
  capture.output(p <- exploreConditionalInferenceTree(subgroup.data = subgroup.data, y_idx = y, stump = FALSE, testtype = "Bonferroni", significance = P_THRESHOLD, nmin = SUBGROUP_MIN_SIZE, maxdepth = MAX_DEPTH))
  plot(main = paste("all covariates - ", colnames(subgroup.data$data)[y]), p)
})

```


You can use the above information to filter down your features. User knowledge is required to decide which features to eliminate. Note that correlated features will cause errors in the downstream analysis.

## Subgroup discovery
Let us discover some subgroups.

```{r subgroup_discovery, message=TRUE, warning=TRUE}
# set the outcomes you want to run subgroup discovery on
outcome_columns <- colnames(subgroup.data$data)[subgroup.data$outcomes]
# save all rules in allRules
allRules <- subgroup.data$rules
allRules

```

### Treatment-specific Subgroup Discovery Tool (TSDT)
<b>
Battioui, C., Shen, L., Ruberg, S., (2014). <i>A Resampling-based Ensemble Tree Method to Identify
Patient Subgroups with Enhanced Treatment Effect.</i> JSM proceedings, 2014 </b>

Increase the number of CPU cores according to your machine.
Increase the sample and permutation in a real world study to at least 100 and 100. The suggested setting by authors is 100 and 500.

<font color="red">Caution: TSDT does not have a built-in model for count data. It assumes it is numeric, and the package adds log transformation on it.</font>

```{r TSDT, message=TRUE, warning=TRUE}
# the true positive / false positive subgroup is
# subgroup.data$rules

param <- list(
  desirable_response = "increasing",
  nsamples = 100, 
  npermutations = 100, # increase the permutation in a real world study 
  maxdepth = MAX_DEPTH, 
  min_subgroup_n_control = SUBGROUP_MIN_SIZE/2, 
  min_subgroup_n_trt = SUBGROUP_MIN_SIZE/2, 
  n_cpu = 1 # increase the number of CPU cores according to your machine
)

resTSDT <- lapply(X = outcome_columns, FUN = function(y_idx) {
  res <- runTSDT(subgroup.data = subgroup.data, 
                     y_idx = y_idx,
                     desirable_response = param$desirable_response,
                     nsamples = param$nsamples, 
                     npermutations = param$npermutations, 
                     maxdepth = param$maxdepth,
                     n_cpu = param$n_cpu)
  allRules <<- rbind(allRules, 
                    parseTSDTResults(TSDT_table = res, outcome = y_idx, param = param, filter = "Strong"))
  return(res)
})

resTSDT

```

### Virtual Twins

Currently only supports binary outcomes and two arm studies.

<b>Foster, Jared C., Jeremy MG Taylor, and Stephen J. Ruberg. <i>Subgroup identification from randomized clinical trial data.<i> Statistics in medicine 30.24 (2011): 2867-2880. 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/21815180">link</a></b>

<a href="https://cran.r-project.org/web/packages/aVirtualTwins/index.html">Link to implemented package</a>


```{R virtual_twins, message=FALSE, warning=FALSE}
# the true positive / false positive subgroup is
# simulated.twoarm$rules

param <- list(
  desirable_response = "increasing", 
  forest.type = "double",
  method = "absolute", 
  tree.type = "class", 
  folds = 10, 
  ntree = 1000,
  maxdepth = 2
)
idx <- sapply(X = outcome_columns, FUN = function(y){
  x <- subgroup.data$data[, y, with = FALSE][[1]]
  is.factor(x) && 
    length(levels(x) == 2)
})
binary_outcomes <- outcome_columns[idx]
resVT <- 
  lapply(X = binary_outcomes,
       FUN = function(y_idx) {
  res <- runVirtualTwins(subgroup.data = subgroup.data, 
                         y_idx = y_idx,
                         desirable_response = param$desirable_response, 
                         forest.type = param$forest.type,
                         method = param$method, 
                         tree.type = param$tree.type, 
                         folds = param$folds, 
                         ntree = param$ntree,
                         maxdepth = param$maxdepth)
  allRules <<- rbind(allRules,
                    parseVTResults(VT_table = res, outcome = y_idx, param = param))
  res
})

resVT

```


### Optimization approach with Particle Swarm Optimization (PSO)
PSO searches for optimal cutoffs while leaxing for p-value and depth. 
You can define your own function or use the default model:
effect size: Y ~ subgroup for one-arm and 
<b>interaction effect size: Y ~ contrast + subgroup + contrast*subgroup for two-arm</b>.

Use larger iteration numbers for larger datasets. 1000 should suffice for up to 100 covariates.

```{r PSO, message=TRUE, warning=TRUE}


# increasing
resPSO <- lapply(X = outcome_columns,
                 FUN = function(y_idx){
  pso <- runPSO(subgroup.data = subgroup.data, 
                y_idx = y_idx,  
                desirable_response = "increasing", 
                depth = MAX_DEPTH, 
                nmin = SUBGROUP_MIN_SIZE, 
                iterations = 1000)
  allRules <<- rbind(allRules, 
                     pso)
  pso
})
resPSO


```


## Analyze and compare rules

```{r compare_rules, echo=FALSE, message=FALSE, warning=FALSE}
threshold <- -log10(P_THRESHOLD)
max_p_value <- -log10(MIN_P_VALUE)
summarizeRules <- function(subgroup.data, rules) {
  N.rules <- nrow(rules)
  summaryRules <- data.table()
  
  allOutcomes <- outcome_columns
  
  for (i in c(1:N.rules)) {
    row <- rules[i]
    # adding a fix to remove dummy rules wihch were negative controls
    if (!grepl(x = row$rule, pattern = "dummy")) {
      if (str_length(row$outcome) == 0) {
        Y <- allOutcomes
      } else {
        Y <- row$outcome
      }
      for (y in Y) {
        row$outcome <- y
        summaryRules <- rbind(summaryRules, row)
      }
    }
    
  }
  N.rules <- nrow(summaryRules)
  N <- nrow(subgroup.data$data)
  effect_size <- sapply(
    X = c(1:N.rules),
    FUN = function(i) {
      row <- summaryRules[i]
      sbg_idx <-
        parseRule(covariates = subgroup.data, rule = row$rule)
      f <- interactionEffectSize(
        subgroup.data = subgroup.data,
        y_idx = row$outcome,
        subgroup_idx = sbg_idx
      )
      c(
        N_ratio = sum(sbg_idx) / N,
        effect = f[1],
        pvalue = f[2]
      )
    }
  )
  summaryRules$N_ratio <- effect_size[1, ]
  summaryRules$effect_size <- effect_size[2, ]
  summaryRules$pvalue <- effect_size[3, ]
  
  # check TP rule and get accuracy precision recall
  truth_subgroup <- which(rules$method == "sim_TP")
  if (length(truth_subgroup) > 0) {
    truth_subgroup <- parseRule(covariates = subgroup.data,
                                rule = rules[truth_subgroup[1], ]$rule)
    performance <- sapply(
      X = c(1:N.rules),
      FUN = function(i) {
        row <- summaryRules[i, ]
        sbg_idx <-
          parseRule(covariates = subgroup.data, rule = row$rule)
        TP <- sum(sbg_idx &  truth_subgroup)
        FP <- sum(sbg_idx & !truth_subgroup)
        TN <- sum(!sbg_idx & !truth_subgroup)
        FN <- sum(!sbg_idx &  truth_subgroup)
        precision <- TP / (TP + FP)
        recall <- TP / (TP + FN)
        F1  <- 2 * (recall * precision) / (precision + recall)
        c(precision = precision,
          recall = recall,
          F1 = F1)
      }
    )
    summaryRules$precision <- performance[1, ] * 100
    summaryRules$recall <- performance[2, ] * 100
    summaryRules$F1 <- performance[3, ] * 100
  }
  return(summaryRules)
}

rules.data <- summarizeRules(subgroup.data = subgroup.data, rules = allRules)
rules.data$log10pvalue <- -log10(rules.data$pvalue)
rules.data$log10pvalue[rules.data$log10pvalue > max_p_value] <- max_p_value


ggplot(
  data = rules.data,
  aes(
    x = precision,
    y = recall,
    shape = outcome,
    color = method,
    size = log10pvalue
  )
) +
geom_point() +
geom_jitter() +
ggtitle("Correctness of rules")

ggplot(
  data = rules.data,
  aes(
    x = effect_size,
    y = log10pvalue,
    shape = outcome,
    color = method,
    size = N_ratio
  )
 
) +
geom_point() +
  geom_jitter() +
ggtitle("Volcano plot of rules") +
geom_hline(yintercept = P_THRESHOLD) +
xlab("contrast*subgroup interaction effect size") + 
ylab("-log10(pvalue)")


rules.data <- rules.data[, c("rule", "outcome", "N_ratio", "effect_size", "pvalue")]
as.data.table(rules.data)

```